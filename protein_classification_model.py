# -*- coding: utf-8 -*-
"""Protein Classification Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNJFxO7yTO4ZQv6f7OsWYJEmZO5SwIlc
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip -q install -U imbalanced-learn
# %pip -q install numpy
# %pip -q install matplotlib
# %pip -q install sklearn
# %pip -q install imblearn

# @title Importing the Libraries
import pandas as pd

import numpy as np

import matplotlib as plt

import csv

from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,f1_score, precision_score, recall_score
from imblearn.over_sampling import SMOTE

"""First we import all the required libraries.

# Cleaning the Dataset
"""

# @title Removing the added rows
data = []
with open('C:/Users/mrdoo/protein_trn_data (4).csv') as file:
    csv_reader = csv.reader(file)
    for row in csv_reader:
        # Check if the number of columns is greater than expected
        if len(row) > 14:
            # Remove the first column and keep only the next 14 columns
            row = row[1:15]

        data.append(row)

ptrn = pd.DataFrame(data)

ptrn.to_csv('ptrn 01.csv', index = False)

"""Here we input the data and check if there are more than 14 rows if yes the rows are shifted to the left."""

def load_data(filepath, head):
    df = pd.read_csv(filepath, low_memory=False, header = head)
    # Add any specific preprocessing steps here (e.g., handling missing values, feature scaling)
    return df

# Load datasets
protrn = load_data('C:/Users/mrdoo/ptrn 01.csv', 1)
plabel = load_data('C:/Users/mrdoo/protein_trn_class_labels (4).csv', None)

protrn.reset_index(drop=True, inplace=True)

protrn

"""Then we load the partially cleaned dataset and the class labels."""

def correct_misalignment(row):
    # Check if structureId has more than 4 characters (indicating misalignment)
    if len(row['structureId']) > 4:
        # Correct the structureId by keeping only the first 4 characters
        # row['structureId'] = row['structureId'][:4]
        # Shift all the subsequent columns to the left
        row[1:-1] = row[2:]
    return row

# Apply the correction to each row
protrn = protrn.apply(correct_misalignment, axis=1)

protrn.to_csv('prottrn.csv', index= False)

ptrn = load_data('C:/Users/mrdoo/prottrn.csv', 0)

"""Then we clean the dataset based on the fact that StructureID should only be 4 cahracters long. If that is not the case the rows are moved to the left."""

plabel.columns = ['Num', 'class']

plabel = plabel.drop('Num', axis= 1)

pr = ptrn.join(plabel, how = 'left')

pr["crystallizationMethod"].replace({"VAPOR DIFFUSION, HANGING DROP" : "VAPOR DIFFUSION",
                                       "VAPOR DIFFUSION, SITTING DROP" : "VAPOR DIFFUSION",
                                       "VAPOR DIFFUSION" : "VAPOR DIFFUSION",
                                       "EVAPORATION" : "EVAPORATION",
                                       "MICROBATCH" : "MICROBATCH",
                                       "hanging drop" : "VAPOR DIFFUSION",
                                       "LIPIDIC CUBIC PHASE" : "LIPIDIC CUBIC PHASE",
                                       "SMALL TUBES" : "Miscellaneous",
                                       "MICRODIALYSIS" : "MICRODIALYSIS",
                                       "BATCH MODE" : "MICROBATCH",
                                       "LIQUID DIFFUSION" : "Miscellaneous",
                                       "VAPOR DIFFUSION,SITTING DROP,NANODROP" : "VAPOR DIFFUSION",
                                       "batch" : "MICROBATCH",
                                       "microbatch under oil" : "MICROBATCH",
                                       "VAPOR DIFFUSION, SITTING DROP, NANODROP" : "VAPOR DIFFUSION"},
                                      inplace=True)

pr['crystallizationMethod'].nunique()

"""Then we drop the index column from the Labels dataset and join the training data and class labels data. Then we do some tuning to reduce uniques in the crystallization method feature."""

pr.nunique()

prr = pr.drop(['publicationYear', 'structureId', 'pdbxDetails'], axis = 1)

"""We drop 3 feature as all of them have different unique values for every data points. This will be not helpful for the model as this does not provide any correlation with different data points."""

imputer = SimpleImputer(strategy='most_frequent')
prr = pd.DataFrame(imputer.fit_transform(prr), columns=prr.columns)

"""We use Simple Imputer to impute the missing values, using the most frequent method."""

prr = prr.dropna(how = 'any')
prr.info()

counts = prr['class'].value_counts()
class_data = np.asarray(counts[(counts > 900)].index)
prn = prr[prr['class'].isin(class_data)]
prn['class'].value_counts()
prn = pd.DataFrame(prn)

"""Then we filter the data based on the class labels as there are labels with less than 200 data points and others with 17000 data points which is causing the precision of the model to be very low."""

#plt.figure(figsize=(8,6))
prn['class'].value_counts()[:100].plot(kind='bar')

"""# Encoding, Splitting and Oversampling the Data"""

labelencoder=LabelEncoder()

for column in prn.columns:
    prn[column] = labelencoder.fit_transform(prn[column])

"""Then we use Label Encoder for encoding as encoding using one-hot method would be quite problematic with the amount of uniques in every feature."""

X = prn.drop('class', axis = 1)
y = prn['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

"""# Training the model

Then we split the data and use SMOTE to oversample the data as there a large amount of imbalance in the dataset.
"""

svm = SVC(kernel = 'linear', C = 1)
rfc = RandomForestClassifier(random_state=42)
knn = KNeighborsClassifier(n_neighbors= 11, leaf_size= 40, metric = 'manhattan', weights = 'distance')
dt = DecisionTreeClassifier(splitter = 'random', min_weight_fraction_leaf = 0.5, max_depth= 7, max_leaf_nodes= 60, min_samples_leaf= 4, min_samples_split= 20)

"""I used four models for training SVM, Random Forest, KNN, Decision Tree"""

def get_metrics(y_test, y_predicted):
    accuracy = accuracy_score(y_test, y_predicted)
    precision = precision_score(y_test, y_predicted, average='weighted')
    recall = recall_score(y_test, y_predicted, average='weighted')
    f1 = f1_score(y_test, y_predicted, average='weighted')
    return accuracy, precision, recall, f1

def model(m):
  m.fit(X_train_smote, y_train_smote.ravel())
  print(round(m.score(X_test,y_test)*100,2))
  predictions = m.predict(X_test)
  accuracy, precision, recall, f1 = get_metrics(y_test, predictions)
  print("accuracy = %.3f \nprecision = %.3f \nrecall = %.3f \nf1 = %.3f" % (accuracy, precision, recall, f1))
  cm = confusion_matrix(y_test, predictions)
  cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)
  cm_display.plot()

models = (rfc, knn, dt)

"""I made a function to use for getting the  train the model, performance metrics and display confusion matrix"""

for x in models:
  n = model(x)

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    # Add other parameters here
}
grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train_smote, y_train_smote)

# Best parameters
print("Best Parameters:", grid_search.best_params_)

# Evaluating the best model
best_rf = grid_search.best_estimator_
y_pred_best = best_rf.predict(X_test)
best_accuracy = accuracy_score(y_test, y_pred_best)
best_precision = precision_score(y_test, y_pred_best)

print(f'Best Model Accuracy: {best_accuracy}, Precision: {best_precision}')

"""As the best model was Random Forest, I did hyperparameter tuning for it."""